{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d26d1c3-f5c4-4ea4-bba1-f2d483242c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install matplotlib\n",
    "# !pip install scipy\n",
    "# !pip install requests\n",
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401146b8-11b8-4c5d-b68f-e808a0d8e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy import stats\n",
    "import ast\n",
    "import os\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1043a86f-46a8-4458-a14d-f233c0fb6a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_21188\\3146588695.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#обработка разовая\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем два списка для каждой категории\n",
    "    in_app_events_dfs = []\n",
    "    fraud_post_inapps_dfs = []\n",
    "    \n",
    "    # Используем эти переменные в начальных значениях\n",
    "    in_app_events_result = pd.DataFrame()\n",
    "    fraud_post_inapps_result = pd.DataFrame()\n",
    "\n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"result_data\"\n",
    "    output_path = os.path.join(file, subfolder)  # Обновленный путь для сохранения файлов\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            in_app_events_dfs.append(df)\n",
    "        elif 'fraud-post-inapps' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            fraud_post_inapps_dfs.append(df)\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        if in_app_events_dfs:\n",
    "            in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'in-app-events' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        if fraud_post_inapps_dfs:\n",
    "            fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'fraud-post-inapps' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(len(in_app_events_result))\n",
    "    print(len(fraud_post_inapps_result))\n",
    "    \n",
    "    # Проверяем наличие столбца 'AppsFlyer ID' в DataFrame in_app_events_result\n",
    "    if 'AppsFlyer ID' in in_app_events_result.columns:\n",
    "           \n",
    "        in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "\n",
    "        drop_duplicates_ = in_app_events_result.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "\n",
    "        # Группируем данные\n",
    "        group_unique_ = drop_duplicates_.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        group_ = in_app_events_result.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        \n",
    "        # Сохраняем результаты в новые xlsx-файлы\n",
    "        with pd.ExcelWriter(f\"{output_path}/in_app_events_result.xlsx\") as writer:\n",
    "            # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "            in_app_events_result.to_excel(writer, sheet_name='in_app_events_result', index=False)\n",
    "\n",
    "            \n",
    "            # Сохраняем результат группировки на втором листе\n",
    "            group_unique_.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "            group_.to_excel(writer, sheet_name='Grouped Data', index=False)    \n",
    "        fraud_post_inapps_result.to_excel(f\"{output_path}/fraud_post_inapps_result.xlsx\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"No 'Platform...' column in 'in_app_events_result' DataFrame\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Определяем значения AppsFlyer ID из fraud_post_inapps_result\n",
    "    fraud_apps_ids = fraud_post_inapps_result['AppsFlyer ID'].unique() if not fraud_post_inapps_result.empty else []\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result передавая значения переменной sorted_date        \n",
    "    in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "    \n",
    "    # Фильтруем строки в in_app_events_result, исключая те, где AppsFlyer ID совпадает с fraud_apps_ids\n",
    "    in_app_events_result_no_fraud = in_app_events_result[~in_app_events_result['AppsFlyer ID'].isin(fraud_apps_ids)]\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле in_app_events_result_no_fraud\n",
    "    drop_duplicates = in_app_events_result_no_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные\n",
    "    group_unique = drop_duplicates.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    group = in_app_events_result_no_fraud.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_no_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_no_fraud.to_excel(writer, sheet_name='No Fraud Data', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Создаем переменную in_app_events_result_highlight_fraud и копируем в нее данные in_app_events_result\n",
    "    in_app_events_result_highlight_fraud = in_app_events_result.copy()\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result_highlight_fraud передавая значения переменной sorted_date        \n",
    "    in_app_events_result_highlight_fraud = in_app_events_result_highlight_fraud.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "    # Отмечаем а не удаляем фроды в отдельном поле\n",
    "    in_app_events_result_highlight_fraud[\"Fraud\"] = in_app_events_result_highlight_fraud['AppsFlyer ID'].isin(fraud_apps_ids)\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле drop_duplicates_highlight_fraud\n",
    "    drop_duplicates_highlight_fraud = in_app_events_result_highlight_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные в лист где уникальные\n",
    "    group_unique_highlight_fraud = drop_duplicates_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    # Группируем данные в лист где не уникальные\n",
    "    group_highlight_fraud = in_app_events_result_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_highlight_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_highlight_fraud.to_excel(writer, sheet_name='highlight_fraud', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique_highlight_fraud.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group_highlight_fraud.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "name_file = \"G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/01.2025/Marathonbet_2025-02-05\"\n",
    "#name_file = \"G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/12.2024/Magnit__2025-01-20\"\n",
    "integration_date(name_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e1e1729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "asdds_\n",
      "as dds_\n"
     ]
    }
   ],
   "source": [
    "a = 'asdds'\n",
    "print(len(a)%2)\n",
    "a = a + '_'\n",
    "print(a)\n",
    "print(a[:2],a[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d36f655-4ffc-4121-addf-38314f65a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4974\n",
      "420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "146\n",
      "No 'in-app-events' data to concatenate\n",
      "No 'fraud-post-inapps' data to concatenate\n",
      "0\n",
      "0\n",
      "No 'Platform...' column in 'in_app_events_result' DataFrame\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'Event Time'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py\u001b[0m in \u001b[0;36m?\u001b[1;34m()\u001b[0m\n\u001b[0;32m    139\u001b[0m             \u001b[0mintegration_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    140\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    141\u001b[0m \u001b[1;31m# Вызов функции\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    142\u001b[0m \u001b[0mroot_folder\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/09.2024/\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 143\u001b[1;33m \u001b[0mprocess_folders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(root_folder)\u001b[0m\n\u001b[0;32m    135\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mprocess_folders\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    136\u001b[0m     \u001b[1;32mfor\u001b[0m \u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfiles\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwalk\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot_folder\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    137\u001b[0m         \u001b[1;32mfor\u001b[0m \u001b[0mdirectory\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdirs\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    138\u001b[0m             \u001b[0mfolder_path\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mroot\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mdirectory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 139\u001b[1;33m             \u001b[0mintegration_date\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfolder_path\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32m~\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     78\u001b[0m     \u001b[1;31m# Определяем значения AppsFlyer ID из fraud_post_inapps_result\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     79\u001b[0m     \u001b[0mfraud_apps_ids\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfraud_post_inapps_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AppsFlyer ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munique\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mfraud_post_inapps_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mempty\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     80\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     81\u001b[0m     \u001b[1;31m# Фильтруем строки в in_app_events_result передавая значения переменной sorted_date\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 82\u001b[1;33m     \u001b[0min_app_events_result\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_app_events_result\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'Event Time'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     83\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     84\u001b[0m     \u001b[1;31m# Фильтруем строки в in_app_events_result, исключая те, где AppsFlyer ID совпадает с fraud_apps_ids\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     85\u001b[0m     \u001b[0min_app_events_result_no_fraud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0min_app_events_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m~\u001b[0m\u001b[0min_app_events_result\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'AppsFlyer ID'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0misin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfraud_apps_ids\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\serge\\f1\\Lib\\site-packages\\pandas\\core\\frame.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, by, axis, ascending, inplace, kind, na_position, ignore_index, key)\u001b[0m\n\u001b[0;32m   7185\u001b[0m             \u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7186\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7187\u001b[0m             \u001b[1;31m# len(by) == 1\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7188\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 7189\u001b[1;33m             \u001b[0mk\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_get_label_or_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mby\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   7190\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7191\u001b[0m             \u001b[1;31m# need to rewrap column in Series to apply key function\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   7192\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mkey\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\Users\\serge\\f1\\Lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36m?\u001b[1;34m(self, key, axis)\u001b[0m\n\u001b[0;32m   1907\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mxs\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mother_axes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1908\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_is_level_reference\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1909\u001b[0m             \u001b[0mvalues\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0maxes\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0maxis\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_level_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_values\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1910\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1911\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1912\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1913\u001b[0m         \u001b[1;31m# Check for duplicates\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1914\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mvalues\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'Event Time'"
     ]
    }
   ],
   "source": [
    "#Обработка всех выгрузок в папке\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем два списка для каждой категории\n",
    "    in_app_events_dfs = []\n",
    "    fraud_post_inapps_dfs = []\n",
    "    \n",
    "    # Используем эти переменные в начальных значениях\n",
    "    in_app_events_result = pd.DataFrame()\n",
    "    fraud_post_inapps_result = pd.DataFrame()\n",
    "\n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"result_data\"\n",
    "    output_path = os.path.join(file, subfolder)  # Обновленный путь для сохранения файлов\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            in_app_events_dfs.append(df)\n",
    "        elif 'fraud-post-inapps' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            fraud_post_inapps_dfs.append(df)\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        if in_app_events_dfs:\n",
    "            in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'in-app-events' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        if fraud_post_inapps_dfs:\n",
    "            fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'fraud-post-inapps' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(len(in_app_events_result))\n",
    "    print(len(fraud_post_inapps_result))\n",
    "    \n",
    "    # Проверяем наличие столбца 'AppsFlyer ID' в DataFrame in_app_events_result\n",
    "    if 'AppsFlyer ID' in in_app_events_result.columns:\n",
    "           \n",
    "        in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "\n",
    "        drop_duplicates_ = in_app_events_result.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "\n",
    "        # Группируем данные\n",
    "        group_unique_ = drop_duplicates_.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        group_ = in_app_events_result.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        \n",
    "        # Сохраняем результаты в новые xlsx-файлы\n",
    "        with pd.ExcelWriter(f\"{output_path}/in_app_events_result.xlsx\") as writer:\n",
    "            # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "            in_app_events_result.to_excel(writer, sheet_name='in_app_events_result', index=False)\n",
    "\n",
    "            \n",
    "            # Сохраняем результат группировки на втором листе\n",
    "            group_unique_.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "            group_.to_excel(writer, sheet_name='Grouped Data', index=False)    \n",
    "        fraud_post_inapps_result.to_excel(f\"{output_path}/fraud_post_inapps_result.xlsx\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"No 'Platform...' column in 'in_app_events_result' DataFrame\")\n",
    "\n",
    "\n",
    "    # Определяем значения AppsFlyer ID из fraud_post_inapps_result\n",
    "    fraud_apps_ids = fraud_post_inapps_result['AppsFlyer ID'].unique() if not fraud_post_inapps_result.empty else []\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result передавая значения переменной sorted_date        \n",
    "    in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "    \n",
    "    # Фильтруем строки в in_app_events_result, исключая те, где AppsFlyer ID совпадает с fraud_apps_ids\n",
    "    in_app_events_result_no_fraud = in_app_events_result[~in_app_events_result['AppsFlyer ID'].isin(fraud_apps_ids)]\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле in_app_events_result_no_fraud\n",
    "    drop_duplicates = in_app_events_result_no_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные\n",
    "    group_unique = drop_duplicates.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    group = in_app_events_result_no_fraud.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_no_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_no_fraud.to_excel(writer, sheet_name='No Fraud Data', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Создаем переменную in_app_events_result_highlight_fraud и копируем в нее данные in_app_events_result\n",
    "    in_app_events_result_highlight_fraud = in_app_events_result.copy()\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result_highlight_fraud передавая значения переменной sorted_date        \n",
    "    in_app_events_result_highlight_fraud = in_app_events_result_highlight_fraud.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "    # Отмечаем а не удаляем фроды в отдельном поле\n",
    "    in_app_events_result_highlight_fraud[\"Fraud\"] = in_app_events_result_highlight_fraud['AppsFlyer ID'].isin(fraud_apps_ids)\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле drop_duplicates_highlight_fraud\n",
    "    drop_duplicates_highlight_fraud = in_app_events_result_highlight_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные в лист где уникальные\n",
    "    group_unique_highlight_fraud = drop_duplicates_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    # Группируем данные в лист где не уникальные\n",
    "    group_highlight_fraud = in_app_events_result_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_highlight_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_highlight_fraud.to_excel(writer, sheet_name='highlight_fraud', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique_highlight_fraud.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group_highlight_fraud.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "\n",
    "def process_folders(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for directory in dirs:\n",
    "            folder_path = os.path.join(root, directory)\n",
    "            integration_date(folder_path)\n",
    "\n",
    "# Вызов функции\n",
    "root_folder = \"G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/09.2024/\"\n",
    "process_folders(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa4c6671-24c0-4811-ae3b-838d69c57583",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "#окно атрибцуии\n",
    "def process_csv(input_path, output_path, filter_value_days, file):\n",
    "    # Чтение xlsx-файла и создание DataFrame\n",
    "    df = pd.read_excel(input_path, parse_dates=['Install Time', 'Event Time'])\n",
    "    \n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"WA\"\n",
    "    output = file\n",
    "    folder_path = os.path.join(output, subfolder)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "    # Вычисление разницы между значениями в столбцах \"Event Time\" и \"Install Time\" в днях\n",
    "    df['Time Difference (Days)'] = (df['Event Time'] - df['Install Time']).dt.days\n",
    "\n",
    "    # Фильтрация данных по заданному значению в днях\n",
    "    filtered_df = df[df['Time Difference (Days)'] < filter_value_days]\n",
    "\n",
    "\n",
    "    # Проверяем наличие столбца 'AppsFlyer ID' в DataFrame in_app_events_result\n",
    "    if 'AppsFlyer ID' in filtered_df.columns:\n",
    "\n",
    "\n",
    "        drop_duplicates_ = filtered_df.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "        \n",
    "        # Группируем данные\n",
    "        group_unique = drop_duplicates_.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        group = filtered_df.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        \n",
    "        # Сохраняем результаты в новые xlsx-файлы\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "            filtered_df.to_excel(writer, sheet_name='in_app_events_result', index=False)\n",
    "\n",
    "            \n",
    "            # Сохраняем результат группировки на втором листе\n",
    "            group_unique.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "            group.to_excel(writer, sheet_name='Grouped Data', index=False)  \n",
    "            \n",
    "    else:\n",
    "        print(\"No 'Platform...' column in 'in_app_events_result' DataFrame\")\n",
    "\n",
    "\n",
    "\n",
    "# Вызов функции\n",
    "file_name = '09.2024/Migcredit_09_2024'\n",
    "file = f'G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/{file_name}/result_data'\n",
    "input_csv_path = f'G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/{file_name}/result_data/in_app_events_result_no_fraud.xlsx'\n",
    "output_csv_path = f'G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/{file_name}/result_data/WA/WA.xlsx'\n",
    "filter_value_days = 30  # Задайте значение в днях\n",
    "\n",
    "process_csv(input_csv_path, output_csv_path, filter_value_days, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5954e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "#обработка разовая агрегированные\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем два списка для каждой категории\n",
    "    agg = []\n",
    "\n",
    "    \n",
    "    # Используем эти переменные в начальных значениях\n",
    "    agg_result = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"result_data\"\n",
    "    output_path = os.path.join(file, subfolder)  # Обновленный путь для сохранения файлов\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(filename)\n",
    "        agg.append(df)\n",
    "    \n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        if agg:\n",
    "            agg_result = pd.concat(agg, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'agg' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    " \n",
    "    print(len(agg_result))\n",
    "    \n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    with pd.ExcelWriter(f\"{output_path}/agg.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        agg_result.to_excel(writer, sheet_name='agg_result', index=False)\n",
    "\n",
    "\n",
    "name_file = \"G:/.shortcut-targets-by-id/1-0y0YvUdxNOM5ZnhM4sE0iJVter1OKIh/тест/AF/08.2024/Fonbet_for_nast_agg\"\n",
    "integration_date(name_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b411f81-9817-4e2b-8fc6-d6f0cb3be716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22792\n",
      "57148\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "malformed node or string on line 1: <ast.Name object at 0x0000016A1526BF50>",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 138\u001b[0m\n\u001b[0;32m    135\u001b[0m     result_retargeting\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/result_data/retargeting_summary.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m    137\u001b[0m name_file \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mC:/Users/Artur/Desktop/AF/585_02_2024\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m--> 138\u001b[0m \u001b[43mintegration_date\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname_file\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mintegration_date\u001b[1;34m(file)\u001b[0m\n\u001b[0;32m     40\u001b[0m retargeting_inapps_result\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/result_data/retargeting_inapps_result.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Используем lambda-функцию для извлечения значения 'af_price' и создания нового столбца\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m in_app_events_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43min_app_events_result\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mEvent Value\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43;01mlambda\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m:\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mfloat\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mget\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43maf_price\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     44\u001b[0m retargeting_inapps_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m retargeting_inapps_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent Value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(ast\u001b[38;5;241m.\u001b[39mliteral_eval(x)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Сохраняем результаты в новые xlsx-файлы\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\series.py:4764\u001b[0m, in \u001b[0;36mSeries.apply\u001b[1;34m(self, func, convert_dtype, args, by_row, **kwargs)\u001b[0m\n\u001b[0;32m   4629\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mapply\u001b[39m(\n\u001b[0;32m   4630\u001b[0m     \u001b[38;5;28mself\u001b[39m,\n\u001b[0;32m   4631\u001b[0m     func: AggFuncType,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4636\u001b[0m     \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[0;32m   4637\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m DataFrame \u001b[38;5;241m|\u001b[39m Series:\n\u001b[0;32m   4638\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   4639\u001b[0m \u001b[38;5;124;03m    Invoke function on values of Series.\u001b[39;00m\n\u001b[0;32m   4640\u001b[0m \n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   4755\u001b[0m \u001b[38;5;124;03m    dtype: float64\u001b[39;00m\n\u001b[0;32m   4756\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m   4757\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mSeriesApply\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   4758\u001b[0m \u001b[43m        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4759\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4760\u001b[0m \u001b[43m        \u001b[49m\u001b[43mconvert_dtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4761\u001b[0m \u001b[43m        \u001b[49m\u001b[43mby_row\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby_row\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4762\u001b[0m \u001b[43m        \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   4763\u001b[0m \u001b[43m        \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m-> 4764\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1209\u001b[0m, in \u001b[0;36mSeriesApply.apply\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1206\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mapply_compat()\n\u001b[0;32m   1208\u001b[0m \u001b[38;5;66;03m# self.func is Callable\u001b[39;00m\n\u001b[1;32m-> 1209\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mapply_standard\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\apply.py:1289\u001b[0m, in \u001b[0;36mSeriesApply.apply_standard\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m   1283\u001b[0m \u001b[38;5;66;03m# row-wise access\u001b[39;00m\n\u001b[0;32m   1284\u001b[0m \u001b[38;5;66;03m# apply doesn't have a `na_action` keyword and for backward compat reasons\u001b[39;00m\n\u001b[0;32m   1285\u001b[0m \u001b[38;5;66;03m# we need to give `na_action=\"ignore\"` for categorical data.\u001b[39;00m\n\u001b[0;32m   1286\u001b[0m \u001b[38;5;66;03m# TODO: remove the `na_action=\"ignore\"` when that default has been changed in\u001b[39;00m\n\u001b[0;32m   1287\u001b[0m \u001b[38;5;66;03m#  Categorical (GH51645).\u001b[39;00m\n\u001b[0;32m   1288\u001b[0m action \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mignore\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(obj\u001b[38;5;241m.\u001b[39mdtype, CategoricalDtype) \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1289\u001b[0m mapped \u001b[38;5;241m=\u001b[39m \u001b[43mobj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_map_values\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1290\u001b[0m \u001b[43m    \u001b[49m\u001b[43mmapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcurried\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maction\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconvert_dtype\u001b[49m\n\u001b[0;32m   1291\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1293\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(mapped) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(mapped[\u001b[38;5;241m0\u001b[39m], ABCSeries):\n\u001b[0;32m   1294\u001b[0m     \u001b[38;5;66;03m# GH#43986 Need to do list(mapped) in order to get treated as nested\u001b[39;00m\n\u001b[0;32m   1295\u001b[0m     \u001b[38;5;66;03m#  See also GH#25959 regarding EA support\u001b[39;00m\n\u001b[0;32m   1296\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m obj\u001b[38;5;241m.\u001b[39m_constructor_expanddim(\u001b[38;5;28mlist\u001b[39m(mapped), index\u001b[38;5;241m=\u001b[39mobj\u001b[38;5;241m.\u001b[39mindex)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\base.py:921\u001b[0m, in \u001b[0;36mIndexOpsMixin._map_values\u001b[1;34m(self, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m    918\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(arr, ExtensionArray):\n\u001b[0;32m    919\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m arr\u001b[38;5;241m.\u001b[39mmap(mapper, na_action\u001b[38;5;241m=\u001b[39mna_action)\n\u001b[1;32m--> 921\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43malgorithms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_array\u001b[49m\u001b[43m(\u001b[49m\u001b[43marr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mna_action\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mna_action\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\site-packages\\pandas\\core\\algorithms.py:1814\u001b[0m, in \u001b[0;36mmap_array\u001b[1;34m(arr, mapper, na_action, convert)\u001b[0m\n\u001b[0;32m   1812\u001b[0m values \u001b[38;5;241m=\u001b[39m arr\u001b[38;5;241m.\u001b[39mastype(\u001b[38;5;28mobject\u001b[39m, copy\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m   1813\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m na_action \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m-> 1814\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mlib\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmap_infer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalues\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconvert\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconvert\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1815\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   1816\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m lib\u001b[38;5;241m.\u001b[39mmap_infer_mask(\n\u001b[0;32m   1817\u001b[0m         values, mapper, mask\u001b[38;5;241m=\u001b[39misna(values)\u001b[38;5;241m.\u001b[39mview(np\u001b[38;5;241m.\u001b[39muint8), convert\u001b[38;5;241m=\u001b[39mconvert\n\u001b[0;32m   1818\u001b[0m     )\n",
      "File \u001b[1;32mlib.pyx:2926\u001b[0m, in \u001b[0;36mpandas._libs.lib.map_infer\u001b[1;34m()\u001b[0m\n",
      "Cell \u001b[1;32mIn[5], line 43\u001b[0m, in \u001b[0;36mintegration_date.<locals>.<lambda>\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     40\u001b[0m retargeting_inapps_result\u001b[38;5;241m.\u001b[39mto_excel(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mfile\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/result_data/retargeting_inapps_result.xlsx\u001b[39m\u001b[38;5;124m\"\u001b[39m, index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m     42\u001b[0m \u001b[38;5;66;03m# Используем lambda-функцию для извлечения значения 'af_price' и создания нового столбца\u001b[39;00m\n\u001b[1;32m---> 43\u001b[0m in_app_events_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m in_app_events_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent Value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(\u001b[43mast\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mliteral_eval\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     44\u001b[0m retargeting_inapps_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m retargeting_inapps_result[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEvent Value\u001b[39m\u001b[38;5;124m'\u001b[39m]\u001b[38;5;241m.\u001b[39mapply(\u001b[38;5;28;01mlambda\u001b[39;00m x: \u001b[38;5;28mfloat\u001b[39m(ast\u001b[38;5;241m.\u001b[39mliteral_eval(x)\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maf_price\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;241m0\u001b[39m)))\n\u001b[0;32m     46\u001b[0m \u001b[38;5;66;03m# Сохраняем результаты в новые xlsx-файлы\u001b[39;00m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py:112\u001b[0m, in \u001b[0;36mliteral_eval\u001b[1;34m(node_or_string)\u001b[0m\n\u001b[0;32m    110\u001b[0m                 \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[0;32m    111\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _convert_signed_num(node)\n\u001b[1;32m--> 112\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_or_string\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py:101\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     99\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mkeys) \u001b[38;5;241m!=\u001b[39m \u001b[38;5;28mlen\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalues):\n\u001b[0;32m    100\u001b[0m         _raise_malformed_node(node)\n\u001b[1;32m--> 101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mdict\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mzip\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkeys\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    102\u001b[0m \u001b[43m                    \u001b[49m\u001b[38;5;28;43mmap\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m_convert\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnode\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvalues\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    103\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, BinOp) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node\u001b[38;5;241m.\u001b[39mop, (Add, Sub)):\n\u001b[0;32m    104\u001b[0m     left \u001b[38;5;241m=\u001b[39m _convert_signed_num(node\u001b[38;5;241m.\u001b[39mleft)\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py:111\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m    109\u001b[0m         \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    110\u001b[0m             \u001b[38;5;28;01mreturn\u001b[39;00m left \u001b[38;5;241m-\u001b[39m right\n\u001b[1;32m--> 111\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_signed_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py:85\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_signed_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     83\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m     84\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;241m-\u001b[39m operand\n\u001b[1;32m---> 85\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_convert_num\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py:76\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._convert_num\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_convert_num\u001b[39m(node):\n\u001b[0;32m     75\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(node, Constant) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(node\u001b[38;5;241m.\u001b[39mvalue) \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;28mint\u001b[39m, \u001b[38;5;28mfloat\u001b[39m, \u001b[38;5;28mcomplex\u001b[39m):\n\u001b[1;32m---> 76\u001b[0m         \u001b[43m_raise_malformed_node\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     77\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m node\u001b[38;5;241m.\u001b[39mvalue\n",
      "File \u001b[1;32m~\\AppData\\Local\\Programs\\Python\\Python312\\Lib\\ast.py:73\u001b[0m, in \u001b[0;36mliteral_eval.<locals>._raise_malformed_node\u001b[1;34m(node)\u001b[0m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m lno \u001b[38;5;241m:=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(node, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlineno\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[0;32m     72\u001b[0m     msg \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m on line \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlno\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m---> 73\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg \u001b[38;5;241m+\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnode\u001b[38;5;132;01m!r}\u001b[39;00m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[1;31mValueError\u001b[0m: malformed node or string on line 1: <ast.Name object at 0x0000016A1526BF50>"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "#retargeting\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    # Создаем два списка для каждой категории\n",
    "    in_app_events_dfs = []\n",
    "    retargeting_inapps_dfs = []\n",
    "\n",
    "    # Используем эти переменные в начальных значениях\n",
    "    in_app_events_result = pd.DataFrame()\n",
    "    retargeting_inapps_result = pd.DataFrame()\n",
    "\n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            in_app_events_dfs.append(df)\n",
    "        elif 'non-organic' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            retargeting_inapps_dfs.append(df)\n",
    "\n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if retargeting_inapps_dfs:\n",
    "            retargeting_inapps_result = pd.concat(retargeting_inapps_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'retargeting-inapps' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    print(len(in_app_events_result))\n",
    "    print(len(retargeting_inapps_result))\n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    in_app_events_result.to_excel(f\"{file}/result_data/in_app_events_result.xlsx\", index=False)\n",
    "    retargeting_inapps_result.to_excel(f\"{file}/result_data/retargeting_inapps_result.xlsx\", index=False)\n",
    "\n",
    "    # Используем lambda-функцию для извлечения значения 'af_price' и создания нового столбца\n",
    "    in_app_events_result['af_price'] = in_app_events_result['Event Value'].apply(lambda x: float(ast.literal_eval(x).get('af_price', 0)))\n",
    "    retargeting_inapps_result['af_price'] = retargeting_inapps_result['Event Value'].apply(lambda x: float(ast.literal_eval(x).get('af_price', 0)))\n",
    "\n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    in_app_events_result.to_excel(f\"{file}/result_data/in_app_events_result.xlsx\", index=False)\n",
    "    retargeting_inapps_result.to_excel(f\"{file}/result_data/retargeting_inapps_result.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # in_app    \n",
    "    #Группируем данные и фильтруем их\n",
    "    # Для af_purchase\n",
    "    G_sum_by_in_app_P = in_app_events_result[in_app_events_result['Event Name'] == 'af_purchase'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "    # Для af_refund\n",
    "    G_sum_by_in_app_R = in_app_events_result[in_app_events_result['Event Name'] == 'af_refund'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Объединяем данные по 'Platform' и 'Media Source' для 'af_purchase' и 'af_refund'\n",
    "    G_in_app_merged_data = pd.merge(G_sum_by_in_app_P, G_sum_by_in_app_R, on=['Platform', 'Media Source'], suffixes=('_P', '_R'), how='left')\n",
    "\n",
    "    # Заменяем все пропущенные значения на 0\n",
    "    G_in_app_merged_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'af_price' для 'af_purchase'\n",
    "    G_in_app_merged_data['Difference'] = abs(G_in_app_merged_data['af_price_P']) - abs(G_in_app_merged_data['af_price_R'])\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'Attributed Touch Type_P' для 'Attributed Touch Type_R'\n",
    "    G_in_app_merged_data['Difference_convers'] = abs(G_in_app_merged_data['Attributed Touch Type_P']) - abs(G_in_app_merged_data['Attributed Touch Type_R'])\n",
    "    print(G_in_app_merged_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # retargeting   \n",
    "    #Группируем данные и фильтруем их\n",
    "    # Для af_purchase\n",
    "    G_sum_by_retargeting_P = retargeting_inapps_result[retargeting_inapps_result['Event Name'] == 'af_purchase'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "    # Для af_refund\n",
    "    G_sum_by_retargeting_R = retargeting_inapps_result[retargeting_inapps_result['Event Name'] == 'af_refund'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "    \n",
    "    # Объединяем данные по 'Platform' и 'Media Source' для 'af_purchase' и 'af_refund'\n",
    "    G_retargeting_merged_data = pd.merge(G_sum_by_retargeting_P, G_sum_by_retargeting_R, on=['Platform', 'Media Source'], suffixes=('_P', '_R'), how='left')\n",
    "\n",
    "    # Заменяем все пропущенные значения на 0\n",
    "    G_retargeting_merged_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'af_price' для 'af_purchase'\n",
    "    G_retargeting_merged_data['Difference'] = abs(G_retargeting_merged_data['af_price_P']) - abs(G_retargeting_merged_data['af_price_R'])\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'Attributed Touch Type_P' для 'Attributed Touch Type_R'\n",
    "    G_retargeting_merged_data['Difference_convers'] = abs(G_retargeting_merged_data['Attributed Touch Type_P']) - abs(G_retargeting_merged_data['Attributed Touch Type_R'])\n",
    "    print(G_retargeting_merged_data)    \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    G_in_app_merged_data.to_excel(f\"{file}/result_data/in_app_group_summary.xlsx\", index=False)\n",
    "    G_retargeting_merged_data.to_excel(f\"{file}/result_data/retargeting_group_summary.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Создаем столбцы с суммами 'af_price' для каждого уникального значения 'Event Name'\n",
    "    # in_app\n",
    "    sum_by_in_app = in_app_events_result.groupby('Event Name')['af_price'].sum().reset_index()\n",
    "    # retargeting\n",
    "    sum_by_retargeting = retargeting_inapps_result.groupby('Event Name')['af_price'].sum().reset_index()\n",
    "\n",
    "    # Создаем столбец 'Difference' как разницу между суммой 'af_purchase' и 'af_refund'\n",
    "    # in_app\n",
    "    in_app_sum_purchase = sum_by_in_app.loc[sum_by_in_app['Event Name'] == 'af_purchase', 'af_price'].values[0]\n",
    "    in_app_sum_refund = sum_by_in_app.loc[sum_by_in_app['Event Name'] == 'af_refund', 'af_price'].values[0]\n",
    "    in_app_difference = abs(in_app_sum_purchase) - abs(in_app_sum_refund)\n",
    "    in_app_percent = abs(in_app_sum_refund)/(abs(in_app_sum_purchase)/100)\n",
    "\n",
    "    # retargeting\n",
    "    retargeting_sum_purchase = sum_by_retargeting.loc[sum_by_retargeting['Event Name'] == 'af_purchase', 'af_price'].values[0]\n",
    "    retargeting_sum_refund = sum_by_retargeting.loc[sum_by_retargeting['Event Name'] == 'af_refund', 'af_price'].values[0]\n",
    "    retargeting_difference = abs(retargeting_sum_purchase) - abs(retargeting_sum_refund)\n",
    "    retargeting_percent = abs(retargeting_sum_refund)/(abs(retargeting_sum_purchase)/100)\n",
    "\n",
    "    # Создаем DataFrame с одной строкой\n",
    "    result_in_app = pd.DataFrame({'sum_purchase': in_app_sum_purchase, 'sum_refund': in_app_sum_refund, 'Difference': in_app_difference, 'procent':in_app_percent}, index=[0])\n",
    "    result_retargeting = pd.DataFrame({'sum_purchase': retargeting_sum_purchase, 'sum_refund': retargeting_sum_refund, 'Difference': retargeting_difference, 'procent':retargeting_percent}, index=[0])\n",
    "\n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    result_in_app.to_excel(f\"{file}/result_data/in_app_summary.xlsx\", index=False)\n",
    "    result_retargeting.to_excel(f\"{file}/result_data/retargeting_summary.xlsx\", index=False)\n",
    "\n",
    "name_file = \"C:/Users/Artur/Desktop/AF/585_02_2024\"\n",
    "integration_date(name_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7536788-9496-4ce4-a5bd-5e0fb461aff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4a12d743-8747-4dc2-b161-9fde0fb1d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "57\n",
      "8\n",
      "7\n",
      "      Media Source  Attributed Touch Type\n",
      "0            XZM10                      1\n",
      "1       gingko_int                      1\n",
      "2       kamier_int                     50\n",
      "3  zhebianwl8q_int                      5\n",
      "      Media Source  Attributed Touch Type\n",
      "0            XZM10                      1\n",
      "1       kamier_int                      4\n",
      "2  zhebianwl8q_int                      2\n",
      "       Media Source  Attributed Touch Type_x  Attributed Touch Type_y\n",
      "0     AS26_d8684c17                        1                      0.0\n",
      "1   blossomad2n_int                       12                      4.0\n",
      "2  dahliamobflg_int                        6                      0.0\n",
      "3        gingko_int                        5                      1.0\n",
      "4     gummymobi_int                       10                      2.0\n",
      "5    spiritmobi_int                        1                      1.0\n",
      "      Media Source  Attributed Touch Type_x  Attributed Touch Type_y\n",
      "0            XZM10                        1                      1.0\n",
      "1       gingko_int                        1                      0.0\n",
      "2       kamier_int                       50                      4.0\n",
      "3  zhebianwl8q_int                        5                      2.0\n",
      "  platform      Media Source  Attributed Touch Type_x  \\\n",
      "0      ios     AS26_d8684c17                        1   \n",
      "1      ios   blossomad2n_int                       12   \n",
      "2      ios  dahliamobflg_int                        6   \n",
      "3      ios        gingko_int                        5   \n",
      "4      ios     gummymobi_int                       10   \n",
      "5      ios    spiritmobi_int                        1   \n",
      "\n",
      "   Attributed Touch Type_y  Difference  \n",
      "0                      0.0         1.0  \n",
      "1                      4.0         8.0  \n",
      "2                      0.0         6.0  \n",
      "3                      1.0         4.0  \n",
      "4                      2.0         8.0  \n",
      "5                      1.0         0.0  \n",
      "  platform     Media Source  Attributed Touch Type_x  Attributed Touch Type_y  \\\n",
      "0  android            XZM10                        1                      1.0   \n",
      "1  android       gingko_int                        1                      0.0   \n",
      "2  android       kamier_int                       50                      4.0   \n",
      "3  android  zhebianwl8q_int                        5                      2.0   \n",
      "\n",
      "   Difference  \n",
      "0         0.0  \n",
      "1         1.0  \n",
      "2        46.0  \n",
      "3         3.0  \n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем списки для каждой категории и пустые датафреймы для результатов\n",
    "    partners_daily_ios = []\n",
    "    partners_daily_android = []\n",
    "    fraud_post_inapps_ios = []\n",
    "    fraud_post_inapps_android = []\n",
    "    partners_daily_result_ios = pd.DataFrame()\n",
    "    partners_daily_result_android = pd.DataFrame()\n",
    "    fraud_post_inapps_result_ios = pd.DataFrame()\n",
    "    fraud_post_inapps_result_android = pd.DataFrame()\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            if 'id' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                partners_daily_ios.append(df)\n",
    "            elif 'ru' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                partners_daily_android.append(df)\n",
    "        elif 'fraud-post-inapps' in filename:\n",
    "            if 'id' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                fraud_post_inapps_ios.append(df)\n",
    "            elif 'ru' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                fraud_post_inapps_android.append(df)\n",
    "    \n",
    "    # Объединяем данные для каждой категории\n",
    "    try:\n",
    "        partners_daily_result_ios = pd.concat(partners_daily_ios, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        partners_daily_result_android = pd.concat(partners_daily_android, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        fraud_post_inapps_result_ios = pd.concat(fraud_post_inapps_ios, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        fraud_post_inapps_result_android = pd.concat(fraud_post_inapps_android, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(len(partners_daily_result_ios))\n",
    "    print(len(partners_daily_result_android))\n",
    "    print(len(fraud_post_inapps_result_ios))\n",
    "    print(len(fraud_post_inapps_result_android))\n",
    "    \n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    partners_daily_result_ios.to_excel(f\"{file}/result_data/partners_daily_result_ios.xlsx\", index=False)\n",
    "    partners_daily_result_android.to_excel(f\"{file}/result_data/partners_daily_result_android.xlsx\", index=False)\n",
    "    fraud_post_inapps_result_ios.to_excel(f\"{file}/result_data/fraud_post_inapps_result_ios.xlsx\", index=False)\n",
    "    fraud_post_inapps_result_android.to_excel(f\"{file}/result_data/fraud_post_inapps_result_android.xlsx\", index=False)\n",
    "\n",
    "    \n",
    "    \"\"\"partners_daily_result_ios.rename(columns={\"Media Source (pid)\":\"Media Source\"},inplace=True)\n",
    "    partners_daily_result_android.rename(columns={\"Media Source (pid)\":\"Media Source\"},inplace=True)\n",
    "\n",
    "    partners_daily_result_ios_by = partners_daily_result_ios.groupby('Media Source')['s2s-cpa-conversion (Unique users)'].sum().reset_index()\n",
    "    partners_daily_result_android_by = partners_daily_result_android.groupby('Media Source')['s2s-cpa-conversion (Unique users)'].sum().reset_index()\n",
    "    fraud_post_inapps_result_ios_by = fraud_post_inapps_result_ios.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    fraud_post_inapps_result_android_by = fraud_post_inapps_result_android.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "\n",
    "    print(partners_daily_result_android_by)\n",
    "    print(fraud_post_inapps_result_android_by)\"\"\"\n",
    "\n",
    "    partners_daily_result_ios_by = partners_daily_result_ios.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    partners_daily_result_android_by = partners_daily_result_android.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    fraud_post_inapps_result_ios_by = fraud_post_inapps_result_ios.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    fraud_post_inapps_result_android_by = fraud_post_inapps_result_android.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "\n",
    "    print(partners_daily_result_android_by)\n",
    "    print(fraud_post_inapps_result_android_by)\n",
    "    \n",
    "    # Объединяем данные по столбцу 'Media Source'\n",
    "    merged_data_ios = pd.merge(partners_daily_result_ios_by, fraud_post_inapps_result_ios_by, on='Media Source', how='left')\n",
    "    merged_data_android = pd.merge(partners_daily_result_android_by, fraud_post_inapps_result_android_by, on='Media Source', how='left')\n",
    "\n",
    "    merged_data_ios.fillna(0, inplace=True)\n",
    "    merged_data_android.fillna(0, inplace=True)\n",
    "\n",
    "    print(merged_data_ios)\n",
    "    print(merged_data_android)\n",
    "    \n",
    "    \"\"\"merged_data_ios['Attributed Touch Type'] = merged_data_ios['Attributed Touch Type'].astype(int)   \n",
    "    merged_data_android['Attributed Touch Type'] = merged_data_android['Attributed Touch Type'].astype(int)\n",
    "\n",
    "\n",
    "    # Вычисляем разность значений\n",
    "    merged_data_ios['Difference'] = merged_data_ios['s2s-cpa-conversion (Unique users)'] - merged_data_ios['Attributed Touch Type']\n",
    "    merged_data_android['Difference'] = merged_data_android['s2s-cpa-conversion (Unique users)'] - merged_data_android['Attributed Touch Type']\"\"\"\n",
    "\n",
    "    merged_data_ios['Difference'] = merged_data_ios['Attributed Touch Type_x'] - merged_data_ios['Attributed Touch Type_y']\n",
    "    merged_data_android['Difference'] = merged_data_android['Attributed Touch Type_x'] - merged_data_android['Attributed Touch Type_y']\n",
    "\n",
    "    \n",
    "    # Добавление столбца 'platform' в merged_data_ios\n",
    "    merged_data_ios.insert(0, 'platform', 'ios')\n",
    "    print(merged_data_ios)\n",
    "    # Добавление столбца 'platform' в merged_data_android\n",
    "    merged_data_android.insert(0, 'platform', 'android')\n",
    "    print(merged_data_android)\n",
    "\n",
    "    # Объединение DataFrame'ов\n",
    "    merged_data = pd.concat([merged_data_ios, merged_data_android], ignore_index=True)\n",
    "    \n",
    "    # Сохраняем только столбец 'Media Source' и столбец 'Difference' в новый xlsx-файл\n",
    "    merged_data.to_excel(f\"{file}/result_data/MR/merged_result.xlsx\", index=False)\n",
    "\n",
    "name_file = \"C:/Users/Artur/Desktop/AF/Банки_ру_01_2024\"\n",
    "integration_date(name_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
