{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3d26d1c3-f5c4-4ea4-bba1-f2d483242c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install pandas\n",
    "# !pip install openpyxl\n",
    "# !pip install matplotlib\n",
    "# !pip install scipy\n",
    "# !pip install requests\n",
    "# !pip install dask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "401146b8-11b8-4c5d-b68f-e808a0d8e735",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "from datetime import datetime\n",
    "import numpy as np\n",
    "#import matplotlib.pyplot as plt\n",
    "#from scipy import stats\n",
    "import ast\n",
    "import os\n",
    "import requests\n",
    "import re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1043a86f-46a8-4458-a14d-f233c0fb6a57",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_21188\\3146588695.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "0\n"
     ]
    }
   ],
   "source": [
    "#обработка разовая\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем два списка для каждой категории\n",
    "    in_app_events_dfs = []\n",
    "    fraud_post_inapps_dfs = []\n",
    "    \n",
    "    # Используем эти переменные в начальных значениях\n",
    "    in_app_events_result = pd.DataFrame()\n",
    "    fraud_post_inapps_result = pd.DataFrame()\n",
    "\n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"result_data\"\n",
    "    output_path = os.path.join(file, subfolder)  # Обновленный путь для сохранения файлов\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            in_app_events_dfs.append(df)\n",
    "        elif 'fraud-post-inapps' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            fraud_post_inapps_dfs.append(df)\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        if in_app_events_dfs:\n",
    "            in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'in-app-events' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        if fraud_post_inapps_dfs:\n",
    "            fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'fraud-post-inapps' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(len(in_app_events_result))\n",
    "    print(len(fraud_post_inapps_result))\n",
    "    \n",
    "    # Проверяем наличие столбца 'AppsFlyer ID' в DataFrame in_app_events_result\n",
    "    if 'AppsFlyer ID' in in_app_events_result.columns:\n",
    "           \n",
    "        in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "\n",
    "        drop_duplicates_ = in_app_events_result.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "\n",
    "        # Группируем данные\n",
    "        group_unique_ = drop_duplicates_.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        group_ = in_app_events_result.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        \n",
    "        # Сохраняем результаты в новые xlsx-файлы\n",
    "        with pd.ExcelWriter(f\"{output_path}/in_app_events_result.xlsx\") as writer:\n",
    "            # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "            in_app_events_result.to_excel(writer, sheet_name='in_app_events_result', index=False)\n",
    "\n",
    "            \n",
    "            # Сохраняем результат группировки на втором листе\n",
    "            group_unique_.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "            group_.to_excel(writer, sheet_name='Grouped Data', index=False)    \n",
    "        fraud_post_inapps_result.to_excel(f\"{output_path}/fraud_post_inapps_result.xlsx\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"No 'Platform...' column in 'in_app_events_result' DataFrame\")\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Определяем значения AppsFlyer ID из fraud_post_inapps_result\n",
    "    fraud_apps_ids = fraud_post_inapps_result['AppsFlyer ID'].unique() if not fraud_post_inapps_result.empty else []\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result передавая значения переменной sorted_date        \n",
    "    in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "    \n",
    "    # Фильтруем строки в in_app_events_result, исключая те, где AppsFlyer ID совпадает с fraud_apps_ids\n",
    "    in_app_events_result_no_fraud = in_app_events_result[~in_app_events_result['AppsFlyer ID'].isin(fraud_apps_ids)]\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле in_app_events_result_no_fraud\n",
    "    drop_duplicates = in_app_events_result_no_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные\n",
    "    group_unique = drop_duplicates.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    group = in_app_events_result_no_fraud.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_no_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_no_fraud.to_excel(writer, sheet_name='No Fraud Data', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Создаем переменную in_app_events_result_highlight_fraud и копируем в нее данные in_app_events_result\n",
    "    in_app_events_result_highlight_fraud = in_app_events_result.copy()\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result_highlight_fraud передавая значения переменной sorted_date        \n",
    "    in_app_events_result_highlight_fraud = in_app_events_result_highlight_fraud.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "    # Отмечаем а не удаляем фроды в отдельном поле\n",
    "    in_app_events_result_highlight_fraud[\"Fraud\"] = in_app_events_result_highlight_fraud['AppsFlyer ID'].isin(fraud_apps_ids)\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле drop_duplicates_highlight_fraud\n",
    "    drop_duplicates_highlight_fraud = in_app_events_result_highlight_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные в лист где уникальные\n",
    "    group_unique_highlight_fraud = drop_duplicates_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    # Группируем данные в лист где не уникальные\n",
    "    group_highlight_fraud = in_app_events_result_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_highlight_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_highlight_fraud.to_excel(writer, sheet_name='highlight_fraud', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique_highlight_fraud.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group_highlight_fraud.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "name_file = \"/AF/01.2025/Marathonbet_2025-02-05\"\n",
    "#name_file = \"/AF/12.2024/Magnit__2025-01-20\"\n",
    "integration_date(name_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "6e1e1729",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "asdds_\n",
      "as dds_\n"
     ]
    }
   ],
   "source": [
    "a = 'asdds'\n",
    "print(len(a)%2)\n",
    "a = a + '_'\n",
    "print(a)\n",
    "print(a[:2],a[2:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3d36f655-4ffc-4121-addf-38314f65a629",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4974\n",
      "420\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:34: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
      "C:\\Users\\serge\\AppData\\Local\\Temp\\ipykernel_57596\\1329086409.py:42: FutureWarning: The behavior of DataFrame concatenation with empty or all-NA entries is deprecated. In a future version, this will no longer exclude empty or all-NA columns when determining the result dtypes. To retain the old behavior, exclude the relevant entries before the concat operation.\n",
      "  fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "676\n",
      "146\n",
      "No 'in-app-events' data to concatenate\n",
      "No 'fraud-post-inapps' data to concatenate\n",
      "0\n",
      "0\n",
      "No 'Platform...' column in 'in_app_events_result' DataFrame\n"
     ]
    },
    {

    }
   ],
   "source": [
    "#Обработка всех выгрузок в папке\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем два списка для каждой категории\n",
    "    in_app_events_dfs = []\n",
    "    fraud_post_inapps_dfs = []\n",
    "    \n",
    "    # Используем эти переменные в начальных значениях\n",
    "    in_app_events_result = pd.DataFrame()\n",
    "    fraud_post_inapps_result = pd.DataFrame()\n",
    "\n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"result_data\"\n",
    "    output_path = os.path.join(file, subfolder)  # Обновленный путь для сохранения файлов\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            in_app_events_dfs.append(df)\n",
    "        elif 'fraud-post-inapps' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            fraud_post_inapps_dfs.append(df)\n",
    "\n",
    "\n",
    "    \n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        if in_app_events_dfs:\n",
    "            in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'in-app-events' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        if fraud_post_inapps_dfs:\n",
    "            fraud_post_inapps_result = pd.concat(fraud_post_inapps_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'fraud-post-inapps' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(len(in_app_events_result))\n",
    "    print(len(fraud_post_inapps_result))\n",
    "    \n",
    "    # Проверяем наличие столбца 'AppsFlyer ID' в DataFrame in_app_events_result\n",
    "    if 'AppsFlyer ID' in in_app_events_result.columns:\n",
    "           \n",
    "        in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "\n",
    "        drop_duplicates_ = in_app_events_result.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "\n",
    "        # Группируем данные\n",
    "        group_unique_ = drop_duplicates_.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        group_ = in_app_events_result.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        \n",
    "        # Сохраняем результаты в новые xlsx-файлы\n",
    "        with pd.ExcelWriter(f\"{output_path}/in_app_events_result.xlsx\") as writer:\n",
    "            # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "            in_app_events_result.to_excel(writer, sheet_name='in_app_events_result', index=False)\n",
    "\n",
    "            \n",
    "            # Сохраняем результат группировки на втором листе\n",
    "            group_unique_.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "            group_.to_excel(writer, sheet_name='Grouped Data', index=False)    \n",
    "        fraud_post_inapps_result.to_excel(f\"{output_path}/fraud_post_inapps_result.xlsx\", index=False)\n",
    "\n",
    "    else:\n",
    "        print(\"No 'Platform...' column in 'in_app_events_result' DataFrame\")\n",
    "\n",
    "\n",
    "    # Определяем значения AppsFlyer ID из fraud_post_inapps_result\n",
    "    fraud_apps_ids = fraud_post_inapps_result['AppsFlyer ID'].unique() if not fraud_post_inapps_result.empty else []\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result передавая значения переменной sorted_date        \n",
    "    in_app_events_result = in_app_events_result.sort_values(by='Event Time', ascending=True)\n",
    "    \n",
    "    # Фильтруем строки в in_app_events_result, исключая те, где AppsFlyer ID совпадает с fraud_apps_ids\n",
    "    in_app_events_result_no_fraud = in_app_events_result[~in_app_events_result['AppsFlyer ID'].isin(fraud_apps_ids)]\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле in_app_events_result_no_fraud\n",
    "    drop_duplicates = in_app_events_result_no_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные\n",
    "    group_unique = drop_duplicates.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    group = in_app_events_result_no_fraud.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_no_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_no_fraud.to_excel(writer, sheet_name='No Fraud Data', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "    # Создаем переменную in_app_events_result_highlight_fraud и копируем в нее данные in_app_events_result\n",
    "    in_app_events_result_highlight_fraud = in_app_events_result.copy()\n",
    "\n",
    "    # Фильтруем строки в in_app_events_result_highlight_fraud передавая значения переменной sorted_date        \n",
    "    in_app_events_result_highlight_fraud = in_app_events_result_highlight_fraud.sort_values(by='Event Time', ascending=True)\n",
    "\n",
    "    # Отмечаем а не удаляем фроды в отдельном поле\n",
    "    in_app_events_result_highlight_fraud[\"Fraud\"] = in_app_events_result_highlight_fraud['AppsFlyer ID'].isin(fraud_apps_ids)\n",
    "\n",
    "    # Создаем переменную где удаляем дубли для отедльного листа в будущем файле drop_duplicates_highlight_fraud\n",
    "    drop_duplicates_highlight_fraud = in_app_events_result_highlight_fraud.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "    \n",
    "    # Группируем данные в лист где уникальные\n",
    "    group_unique_highlight_fraud = drop_duplicates_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    # Группируем данные в лист где не уникальные\n",
    "    group_highlight_fraud = in_app_events_result_highlight_fraud.groupby(['Platform','Media Source', 'Country Code', 'Fraud'])['Attributed Touch Type'].count().reset_index()\n",
    "    \n",
    "    # Создаем объект ExcelWriter для записи в один файл Excel\n",
    "    with pd.ExcelWriter(f\"{output_path}/in_app_events_result_highlight_fraud.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        in_app_events_result_highlight_fraud.to_excel(writer, sheet_name='highlight_fraud', index=False)\n",
    "\n",
    "        # Сохраняем результат группировки на втором листе\n",
    "        group_unique_highlight_fraud.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "        \n",
    "        group_highlight_fraud.to_excel(writer, sheet_name='Grouped Data', index=False)\n",
    "\n",
    "\n",
    "def process_folders(root_folder):\n",
    "    for root, dirs, files in os.walk(root_folder):\n",
    "        for directory in dirs:\n",
    "            folder_path = os.path.join(root, directory)\n",
    "            integration_date(folder_path)\n",
    "\n",
    "# Вызов функции\n",
    "root_folder = \"/AF/09.2024/\"\n",
    "process_folders(root_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "fa4c6671-24c0-4811-ae3b-838d69c57583",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "#окно атрибцуии\n",
    "def process_csv(input_path, output_path, filter_value_days, file):\n",
    "    # Чтение xlsx-файла и создание DataFrame\n",
    "    df = pd.read_excel(input_path, parse_dates=['Install Time', 'Event Time'])\n",
    "    \n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"WA\"\n",
    "    output = file\n",
    "    folder_path = os.path.join(output, subfolder)\n",
    "    if not os.path.exists(folder_path):\n",
    "        os.makedirs(folder_path)\n",
    "        \n",
    "    # Вычисление разницы между значениями в столбцах \"Event Time\" и \"Install Time\" в днях\n",
    "    df['Time Difference (Days)'] = (df['Event Time'] - df['Install Time']).dt.days\n",
    "\n",
    "    # Фильтрация данных по заданному значению в днях\n",
    "    filtered_df = df[df['Time Difference (Days)'] < filter_value_days]\n",
    "\n",
    "\n",
    "    # Проверяем наличие столбца 'AppsFlyer ID' в DataFrame in_app_events_result\n",
    "    if 'AppsFlyer ID' in filtered_df.columns:\n",
    "\n",
    "\n",
    "        drop_duplicates_ = filtered_df.drop_duplicates(subset=['AppsFlyer ID'])\n",
    "        \n",
    "        # Группируем данные\n",
    "        group_unique = drop_duplicates_.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        group = filtered_df.groupby(['Platform','Media Source', 'Country Code'])['Attributed Touch Type'].count().reset_index()\n",
    "        \n",
    "        # Сохраняем результаты в новые xlsx-файлы\n",
    "        with pd.ExcelWriter(output_path) as writer:\n",
    "            # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "            filtered_df.to_excel(writer, sheet_name='in_app_events_result', index=False)\n",
    "\n",
    "            \n",
    "            # Сохраняем результат группировки на втором листе\n",
    "            group_unique.to_excel(writer, sheet_name='Grouped Data (unique)', index=False)\n",
    "            group.to_excel(writer, sheet_name='Grouped Data', index=False)  \n",
    "            \n",
    "    else:\n",
    "        print(\"No 'Platform...' column in 'in_app_events_result' DataFrame\")\n",
    "\n",
    "\n",
    "\n",
    "# Вызов функции\n",
    "file_name = '09.2024/Migcredit_09_2024'\n",
    "file = f'/AF/{file_name}/result_data'\n",
    "input_csv_path = f'/AF/{file_name}/result_data/in_app_events_result_no_fraud.xlsx'\n",
    "output_csv_path = f'/AF/{file_name}/result_data/WA/WA.xlsx'\n",
    "filter_value_days = 30  # Задайте значение в днях\n",
    "\n",
    "process_csv(input_csv_path, output_csv_path, filter_value_days, file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a5954e8",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "#обработка разовая агрегированные\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем два списка для каждой категории\n",
    "    agg = []\n",
    "\n",
    "    \n",
    "    # Используем эти переменные в начальных значениях\n",
    "    agg_result = pd.DataFrame()\n",
    "\n",
    "\n",
    "    # Проверка существования папки, если нет, то создаем\n",
    "    subfolder = \"result_data\"\n",
    "    output_path = os.path.join(file, subfolder)  # Обновленный путь для сохранения файлов\n",
    "    if not os.path.exists(output_path):\n",
    "        os.makedirs(output_path)\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        df = pd.read_csv(filename)\n",
    "        agg.append(df)\n",
    "    \n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        if agg:\n",
    "            agg_result = pd.concat(agg, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'agg' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    " \n",
    "    print(len(agg_result))\n",
    "    \n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    with pd.ExcelWriter(f\"{output_path}/agg.xlsx\") as writer:\n",
    "        # Сохраняем данные без мошеннических идентификаторов на первом листе\n",
    "        agg_result.to_excel(writer, sheet_name='agg_result', index=False)\n",
    "\n",
    "\n",
    "name_file = \"/AF/08.2024/Fonbet_for_nast_agg\"\n",
    "integration_date(name_file)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9b411f81-9817-4e2b-8fc6-d6f0cb3be716",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22792\n",
      "57148\n"
     ]
    },
    {

     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "#retargeting\n",
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "\n",
    "    # Создаем два списка для каждой категории\n",
    "    in_app_events_dfs = []\n",
    "    retargeting_inapps_dfs = []\n",
    "\n",
    "    # Используем эти переменные в начальных значениях\n",
    "    in_app_events_result = pd.DataFrame()\n",
    "    retargeting_inapps_result = pd.DataFrame()\n",
    "\n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            in_app_events_dfs.append(df)\n",
    "        elif 'non-organic' in filename:\n",
    "            df = pd.read_csv(filename)\n",
    "            retargeting_inapps_dfs.append(df)\n",
    "\n",
    "    try:\n",
    "        # Используем pd.concat для каждой категории\n",
    "        in_app_events_result = pd.concat(in_app_events_dfs, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "\n",
    "    try:\n",
    "        if retargeting_inapps_dfs:\n",
    "            retargeting_inapps_result = pd.concat(retargeting_inapps_dfs, axis=0, ignore_index=True)\n",
    "        else:\n",
    "            raise pd.errors.EmptyDataError(\"No 'retargeting-inapps' data to concatenate\")\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    print(len(in_app_events_result))\n",
    "    print(len(retargeting_inapps_result))\n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    in_app_events_result.to_excel(f\"{file}/result_data/in_app_events_result.xlsx\", index=False)\n",
    "    retargeting_inapps_result.to_excel(f\"{file}/result_data/retargeting_inapps_result.xlsx\", index=False)\n",
    "\n",
    "    # Используем lambda-функцию для извлечения значения 'af_price' и создания нового столбца\n",
    "    in_app_events_result['af_price'] = in_app_events_result['Event Value'].apply(lambda x: float(ast.literal_eval(x).get('af_price', 0)))\n",
    "    retargeting_inapps_result['af_price'] = retargeting_inapps_result['Event Value'].apply(lambda x: float(ast.literal_eval(x).get('af_price', 0)))\n",
    "\n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    in_app_events_result.to_excel(f\"{file}/result_data/in_app_events_result.xlsx\", index=False)\n",
    "    retargeting_inapps_result.to_excel(f\"{file}/result_data/retargeting_inapps_result.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # in_app    \n",
    "    #Группируем данные и фильтруем их\n",
    "    # Для af_purchase\n",
    "    G_sum_by_in_app_P = in_app_events_result[in_app_events_result['Event Name'] == 'af_purchase'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "    # Для af_refund\n",
    "    G_sum_by_in_app_R = in_app_events_result[in_app_events_result['Event Name'] == 'af_refund'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    # Объединяем данные по 'Platform' и 'Media Source' для 'af_purchase' и 'af_refund'\n",
    "    G_in_app_merged_data = pd.merge(G_sum_by_in_app_P, G_sum_by_in_app_R, on=['Platform', 'Media Source'], suffixes=('_P', '_R'), how='left')\n",
    "\n",
    "    # Заменяем все пропущенные значения на 0\n",
    "    G_in_app_merged_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'af_price' для 'af_purchase'\n",
    "    G_in_app_merged_data['Difference'] = abs(G_in_app_merged_data['af_price_P']) - abs(G_in_app_merged_data['af_price_R'])\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'Attributed Touch Type_P' для 'Attributed Touch Type_R'\n",
    "    G_in_app_merged_data['Difference_convers'] = abs(G_in_app_merged_data['Attributed Touch Type_P']) - abs(G_in_app_merged_data['Attributed Touch Type_R'])\n",
    "    print(G_in_app_merged_data)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    # retargeting   \n",
    "    #Группируем данные и фильтруем их\n",
    "    # Для af_purchase\n",
    "    G_sum_by_retargeting_P = retargeting_inapps_result[retargeting_inapps_result['Event Name'] == 'af_purchase'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "    # Для af_refund\n",
    "    G_sum_by_retargeting_R = retargeting_inapps_result[retargeting_inapps_result['Event Name'] == 'af_refund'].groupby(['Platform', 'Media Source']).agg({'af_price': 'sum', 'Attributed Touch Type': 'count'}).reset_index()\n",
    "    \n",
    "    # Объединяем данные по 'Platform' и 'Media Source' для 'af_purchase' и 'af_refund'\n",
    "    G_retargeting_merged_data = pd.merge(G_sum_by_retargeting_P, G_sum_by_retargeting_R, on=['Platform', 'Media Source'], suffixes=('_P', '_R'), how='left')\n",
    "\n",
    "    # Заменяем все пропущенные значения на 0\n",
    "    G_retargeting_merged_data.fillna(0, inplace=True)\n",
    "\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'af_price' для 'af_purchase'\n",
    "    G_retargeting_merged_data['Difference'] = abs(G_retargeting_merged_data['af_price_P']) - abs(G_retargeting_merged_data['af_price_R'])\n",
    "    # Вычитаем сумму 'af_price' для 'af_refund' из суммы 'Attributed Touch Type_P' для 'Attributed Touch Type_R'\n",
    "    G_retargeting_merged_data['Difference_convers'] = abs(G_retargeting_merged_data['Attributed Touch Type_P']) - abs(G_retargeting_merged_data['Attributed Touch Type_R'])\n",
    "    print(G_retargeting_merged_data)    \n",
    "\n",
    "   \n",
    "    \n",
    "    \n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    G_in_app_merged_data.to_excel(f\"{file}/result_data/in_app_group_summary.xlsx\", index=False)\n",
    "    G_retargeting_merged_data.to_excel(f\"{file}/result_data/retargeting_group_summary.xlsx\", index=False)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    # Создаем столбцы с суммами 'af_price' для каждого уникального значения 'Event Name'\n",
    "    # in_app\n",
    "    sum_by_in_app = in_app_events_result.groupby('Event Name')['af_price'].sum().reset_index()\n",
    "    # retargeting\n",
    "    sum_by_retargeting = retargeting_inapps_result.groupby('Event Name')['af_price'].sum().reset_index()\n",
    "\n",
    "    # Создаем столбец 'Difference' как разницу между суммой 'af_purchase' и 'af_refund'\n",
    "    # in_app\n",
    "    in_app_sum_purchase = sum_by_in_app.loc[sum_by_in_app['Event Name'] == 'af_purchase', 'af_price'].values[0]\n",
    "    in_app_sum_refund = sum_by_in_app.loc[sum_by_in_app['Event Name'] == 'af_refund', 'af_price'].values[0]\n",
    "    in_app_difference = abs(in_app_sum_purchase) - abs(in_app_sum_refund)\n",
    "    in_app_percent = abs(in_app_sum_refund)/(abs(in_app_sum_purchase)/100)\n",
    "\n",
    "    # retargeting\n",
    "    retargeting_sum_purchase = sum_by_retargeting.loc[sum_by_retargeting['Event Name'] == 'af_purchase', 'af_price'].values[0]\n",
    "    retargeting_sum_refund = sum_by_retargeting.loc[sum_by_retargeting['Event Name'] == 'af_refund', 'af_price'].values[0]\n",
    "    retargeting_difference = abs(retargeting_sum_purchase) - abs(retargeting_sum_refund)\n",
    "    retargeting_percent = abs(retargeting_sum_refund)/(abs(retargeting_sum_purchase)/100)\n",
    "\n",
    "    # Создаем DataFrame с одной строкой\n",
    "    result_in_app = pd.DataFrame({'sum_purchase': in_app_sum_purchase, 'sum_refund': in_app_sum_refund, 'Difference': in_app_difference, 'procent':in_app_percent}, index=[0])\n",
    "    result_retargeting = pd.DataFrame({'sum_purchase': retargeting_sum_purchase, 'sum_refund': retargeting_sum_refund, 'Difference': retargeting_difference, 'procent':retargeting_percent}, index=[0])\n",
    "\n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    result_in_app.to_excel(f\"{file}/result_data/in_app_summary.xlsx\", index=False)\n",
    "    result_retargeting.to_excel(f\"{file}/result_data/retargeting_summary.xlsx\", index=False)\n",
    "\n",
    "name_file = \"C:/Users/Artur/Desktop/AF/585_02_2024\"\n",
    "integration_date(name_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7536788-9496-4ce4-a5bd-5e0fb461aff2",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 215,
   "id": "4a12d743-8747-4dc2-b161-9fde0fb1d774",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "35\n",
      "57\n",
      "8\n",
      "7\n",

     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mnotebook controller is DISPOSED. \n",
      "\u001b[1;31mПодробнее см. в <a href='command:jupyter.viewOutput'>журнале Jupyter</a>."
     ]
    }
   ],
   "source": [
    "def integration_date(file):\n",
    "    path = f'{file}'\n",
    "    filenames = glob.glob(path + \"/*.csv\")\n",
    "    \n",
    "    # Создаем списки для каждой категории и пустые датафреймы для результатов\n",
    "    partners_daily_ios = []\n",
    "    partners_daily_android = []\n",
    "    fraud_post_inapps_ios = []\n",
    "    fraud_post_inapps_android = []\n",
    "    partners_daily_result_ios = pd.DataFrame()\n",
    "    partners_daily_result_android = pd.DataFrame()\n",
    "    fraud_post_inapps_result_ios = pd.DataFrame()\n",
    "    fraud_post_inapps_result_android = pd.DataFrame()\n",
    "    \n",
    "    # Проходим по всем файлам и добавляем их данные в соответствующий список\n",
    "    for filename in filenames:\n",
    "        if 'in-app-events' in filename:\n",
    "            if 'id' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                partners_daily_ios.append(df)\n",
    "            elif 'ru' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                partners_daily_android.append(df)\n",
    "        elif 'fraud-post-inapps' in filename:\n",
    "            if 'id' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                fraud_post_inapps_ios.append(df)\n",
    "            elif 'ru' in filename:\n",
    "                df = pd.read_csv(filename)\n",
    "                fraud_post_inapps_android.append(df)\n",
    "    \n",
    "    # Объединяем данные для каждой категории\n",
    "    try:\n",
    "        partners_daily_result_ios = pd.concat(partners_daily_ios, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        partners_daily_result_android = pd.concat(partners_daily_android, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        fraud_post_inapps_result_ios = pd.concat(fraud_post_inapps_ios, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    try:\n",
    "        fraud_post_inapps_result_android = pd.concat(fraud_post_inapps_android, axis=0, ignore_index=True)\n",
    "    except pd.errors.EmptyDataError as e:\n",
    "        print(e)\n",
    "    \n",
    "    print(len(partners_daily_result_ios))\n",
    "    print(len(partners_daily_result_android))\n",
    "    print(len(fraud_post_inapps_result_ios))\n",
    "    print(len(fraud_post_inapps_result_android))\n",
    "    \n",
    "    # Сохраняем результаты в новые xlsx-файлы\n",
    "    partners_daily_result_ios.to_excel(f\"{file}/result_data/partners_daily_result_ios.xlsx\", index=False)\n",
    "    partners_daily_result_android.to_excel(f\"{file}/result_data/partners_daily_result_android.xlsx\", index=False)\n",
    "    fraud_post_inapps_result_ios.to_excel(f\"{file}/result_data/fraud_post_inapps_result_ios.xlsx\", index=False)\n",
    "    fraud_post_inapps_result_android.to_excel(f\"{file}/result_data/fraud_post_inapps_result_android.xlsx\", index=False)\n",
    "\n",
    "    \n",
    "    \"\"\"partners_daily_result_ios.rename(columns={\"Media Source (pid)\":\"Media Source\"},inplace=True)\n",
    "    partners_daily_result_android.rename(columns={\"Media Source (pid)\":\"Media Source\"},inplace=True)\n",
    "\n",
    "    partners_daily_result_ios_by = partners_daily_result_ios.groupby('Media Source')['s2s-cpa-conversion (Unique users)'].sum().reset_index()\n",
    "    partners_daily_result_android_by = partners_daily_result_android.groupby('Media Source')['s2s-cpa-conversion (Unique users)'].sum().reset_index()\n",
    "    fraud_post_inapps_result_ios_by = fraud_post_inapps_result_ios.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    fraud_post_inapps_result_android_by = fraud_post_inapps_result_android.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "\n",
    "    print(partners_daily_result_android_by)\n",
    "    print(fraud_post_inapps_result_android_by)\"\"\"\n",
    "\n",
    "    partners_daily_result_ios_by = partners_daily_result_ios.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    partners_daily_result_android_by = partners_daily_result_android.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    fraud_post_inapps_result_ios_by = fraud_post_inapps_result_ios.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "    fraud_post_inapps_result_android_by = fraud_post_inapps_result_android.groupby('Media Source')['Attributed Touch Type'].count().reset_index()\n",
    "\n",
    "    print(partners_daily_result_android_by)\n",
    "    print(fraud_post_inapps_result_android_by)\n",
    "    \n",
    "    # Объединяем данные по столбцу 'Media Source'\n",
    "    merged_data_ios = pd.merge(partners_daily_result_ios_by, fraud_post_inapps_result_ios_by, on='Media Source', how='left')\n",
    "    merged_data_android = pd.merge(partners_daily_result_android_by, fraud_post_inapps_result_android_by, on='Media Source', how='left')\n",
    "\n",
    "    merged_data_ios.fillna(0, inplace=True)\n",
    "    merged_data_android.fillna(0, inplace=True)\n",
    "\n",
    "    print(merged_data_ios)\n",
    "    print(merged_data_android)\n",
    "    \n",
    "    \"\"\"merged_data_ios['Attributed Touch Type'] = merged_data_ios['Attributed Touch Type'].astype(int)   \n",
    "    merged_data_android['Attributed Touch Type'] = merged_data_android['Attributed Touch Type'].astype(int)\n",
    "\n",
    "\n",
    "    # Вычисляем разность значений\n",
    "    merged_data_ios['Difference'] = merged_data_ios['s2s-cpa-conversion (Unique users)'] - merged_data_ios['Attributed Touch Type']\n",
    "    merged_data_android['Difference'] = merged_data_android['s2s-cpa-conversion (Unique users)'] - merged_data_android['Attributed Touch Type']\"\"\"\n",
    "\n",
    "    merged_data_ios['Difference'] = merged_data_ios['Attributed Touch Type_x'] - merged_data_ios['Attributed Touch Type_y']\n",
    "    merged_data_android['Difference'] = merged_data_android['Attributed Touch Type_x'] - merged_data_android['Attributed Touch Type_y']\n",
    "\n",
    "    \n",
    "    # Добавление столбца 'platform' в merged_data_ios\n",
    "    merged_data_ios.insert(0, 'platform', 'ios')\n",
    "    print(merged_data_ios)\n",
    "    # Добавление столбца 'platform' в merged_data_android\n",
    "    merged_data_android.insert(0, 'platform', 'android')\n",
    "    print(merged_data_android)\n",
    "\n",
    "    # Объединение DataFrame'ов\n",
    "    merged_data = pd.concat([merged_data_ios, merged_data_android], ignore_index=True)\n",
    "    \n",
    "    # Сохраняем только столбец 'Media Source' и столбец 'Difference' в новый xlsx-файл\n",
    "    merged_data.to_excel(f\"{file}/result_data/MR/merged_result.xlsx\", index=False)\n",
    "\n",
    "name_file = \"C:/Users/Artur/Desktop/AF/Банки_ру_01_2024\"\n",
    "integration_date(name_file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "f1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
